version: "3.4"
services:
    giveme5w1h-proxy-service:
        build: .
        image: giveme5w1h-proxy
        networks:
            - giveme5w1h-proxy-network
        ports:
            - 9099:9099
        environment:
            - CORE_NLP_SERVER_HOST=corenlp-service
        deploy:
            replicas: 1
            restart_policy:
                condition: any
    corenlp-service:
        image: nlpbox/corenlp:3.8.0
        networks:
            - giveme5w1h-proxy-network
        ports:
            - 9000:9000
        command: java -Xms2g -Xmx3g -cp "*" edu.stanford.nlp.pipeline.StanfordCoreNLPServer --timeout 15000 --preload "tokenize,ssplit,pos,lemma,parse,ner,depparse,mention,coref"
        healthcheck:
            test: wget --quiet --spider --tries=1 --post-data "The quick brown fox jumped over the lazy dog." "http://localhost:9000/?properties={%22annotators%22%3A%22tokenize%2Cssplit%2Cpos%22%2C%22outputFormat%22%3A%22json%22}" || exit 1
            interval: 30s
            timeout: 15s
            retries: 3
            start_period: 2m
        deploy:
            replicas: 2
            restart_policy:
                condition: any
networks:
    giveme5w1h-proxy-network:
        driver: overlay
