{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import sklearn.model_selection as sk\n",
    "from whatwhy import QUESTION_WORDS\n",
    "from whatwhy.text_processing.helper_methods import get_df_from_file\n",
    "%run \"/home/stevengt/Documents/code/whatwhy/notebooks/Tweet CSV Consolidation.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-12d9e5a419b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgensim_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-wiki-gigaword-50\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# gensim_model = gensim.models.KeyedVectors.load_word2vec_format(\"/home/stevengt/Downloads/GoogleNews-vectors-negative300.bin\", binary=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# gensim_model = gensim.downloader.load(\"glove-wiki-gigaword-50\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/whatwhy/lib/python3.5/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/glove-wiki-gigaword-50/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-wiki-gigaword-50'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-wiki-gigaword-50.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/whatwhy/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/whatwhy/lib/python3.5/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/whatwhy/lib/python3.5/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "gensim_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# gensim_model = gensim.models.KeyedVectors.load_word2vec_format(\"/home/stevengt/Downloads/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# gensim_model = gensim.downloader.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 10533772564491166947), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5150765601722366644)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Configure keras to use the plaidml backend in order to use an AMD GPU.\n",
    "import ngraph_bridge\n",
    "from tensorflow.keras import backend as K\n",
    "# ngraph_bridge.set_backend('PLAIDML')\n",
    "config = tf.ConfigProto()\n",
    "\n",
    "# backend_name = \"PLAIDML\" \n",
    "# device_id = \"opencl_amd_hawaii.0\" \n",
    "# opt_name = 'ngraph-optimizer' \n",
    "# from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "# rewriter_options = rewriter_config_pb2.RewriterConfig() \n",
    "# rewriter_options.meta_optimizer_iterations=(rewriter_config_pb2.RewriterConfig.ONE) \n",
    "# rewriter_options.min_graph_nodes=-1 \n",
    "# ngraph_optimizer = rewriter_options.custom_optimizers.add() \n",
    "# ngraph_optimizer.name = opt_name \n",
    "# ngraph_optimizer.parameter_map[\"ngraph_backend\"].s = backend_name.encode() \n",
    "# ngraph_optimizer.parameter_map[\"device_id\"].s = device_id.encode() \n",
    "# config.MergeFrom(tf.ConfigProto(graph_options=tf.GraphOptions(rewrite_options=rewriter_options))) \n",
    "\n",
    "\n",
    "# config_ngraph_enabled = ngraph_bridge.update_config(config, backend_name=\"PLAIDML\", device_id=\"opencl_amd_hawaii.0\" ) \n",
    "with tf.Session(config=config_ngraph_enabled).as_default() as sess:\n",
    "    K.set_session(sess)\n",
    "    # from tensorflow.python.client import device_lib\n",
    "    # print(device_lib.list_local_devices())\n",
    "    print(sess.list_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_as_list(text):\n",
    "    if text is None or text is np.nan:\n",
    "        return []\n",
    "    else:\n",
    "        return ast.literal_eval(text)\n",
    "\n",
    "def get_max_num_tokens(all_tokens):\n",
    "    return max( [ len(tokens_list) for tokens_list in all_tokens ] )\n",
    "\n",
    "def get_max_token_length(all_tokens):\n",
    "    longest_tokens = [0]\n",
    "    for tokens_list in all_tokens:\n",
    "        if len(tokens_list) != 0:\n",
    "            longest_tokens.append( max([ len(token) for token in tokens_list ]) )\n",
    "    return max(longest_tokens)\n",
    "\n",
    "def get_num_words_in_vocab(gensim_model):\n",
    "    return len(gensim_model.vocab.keys())\n",
    "\n",
    "def unflatten_vector(flattened_vector, max_num_tokens, vector_size):\n",
    "    return flattened_vector.reshape([max_num_tokens, vector_size])\n",
    "\n",
    "def convert_embedded_phrase_to_english(gensim_model, embedded_phrase_vector, max_num_tokens):\n",
    "    words = []\n",
    "    vector_size = gensim_model.vector_size\n",
    "    embedded_word_vectors = unflatten_vector(gensim_model, embedded_phrase_vector, max_num_tokens, vector_size)\n",
    "    for i in range(max_num_tokens):\n",
    "        embedded_word_vector = embedded_word_vectors[i,:]\n",
    "        word = gensim_model.similar_by_vector(embedded_word_vector, topn=1)[0][0]\n",
    "        words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "def convert_one_hot_gensim_model_indeces_to_english(gensim_model, one_hot_indeces, max_num_tokens):\n",
    "    words = []\n",
    "    vector_size = get_num_words_in_vocab(gensim_model)\n",
    "    word_indeces = unflatten_vector(gensim_model, one_hot_indeces, max_num_tokens, vector_size)\n",
    "    for i in range(max_num_tokens):\n",
    "        word_index_one_hot = word_indeces[i,:]\n",
    "        if np.count_nonzero(word_index_one_hot) > 0:\n",
    "            word_index = np.argmax(word_index_one_hot)\n",
    "            word = gensim_model.index2word[word_index]\n",
    "            words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def embed_list_of_tokens(gensim_model, tokens, max_num_tokens):\n",
    "    vector_size = gensim_model.vector_size\n",
    "    embedded_tokens = np.zeros([max_num_tokens, vector_size])\n",
    "    cur_index = 0\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embedded_tokens[cur_index, :] = gensim_model.get_vector(token)\n",
    "            cur_index += 1\n",
    "        except:\n",
    "            pass\n",
    "    return embedded_tokens\n",
    "\n",
    "def embed_lists_of_tokens(gensim_model, tokens_lists):\n",
    "    max_num_tokens = get_max_num_tokens(tokens_lists)\n",
    "    num_token_lists = len(tokens_lists)\n",
    "    num_tokens_per_sample = max_num_tokens\n",
    "    token_vector_size = gensim_model.vector_size\n",
    "    \n",
    "    embedded_tokens = np.zeros([num_token_lists, num_tokens_per_sample, token_vector_size])\n",
    "\n",
    "    for i, tokens_list in enumerate(tokens_lists):\n",
    "        embedded_tokens[i,:,:] = embed_list_of_tokens(gensim_model, tokens_list, max_num_tokens)\n",
    "    return embedded_tokens\n",
    "\n",
    "\n",
    "\n",
    "def get_integer_indeces_from_tokens_lists(gensim_model, tokens_lists):\n",
    "    max_num_tokens = get_max_num_tokens(tokens_lists)\n",
    "    num_lists = len(tokens_lists)\n",
    "    \n",
    "    # Use a default value of -1 because 0 will be one-hot encoded, but not -1.\n",
    "    indeces = -1 * np.ones([num_lists, max_num_tokens])\n",
    "    for i, tokens_list in enumerate(tokens_lists):\n",
    "        j = 0\n",
    "        for token in tokens_list:\n",
    "            try:\n",
    "                indeces[i,j] = gensim_model.vocab[token].index\n",
    "                j += 1\n",
    "            except:\n",
    "                continue\n",
    "    return indeces\n",
    "\n",
    "def get_flattened_one_hot_encodings_from_tokens_lists(gensim_model, tokens_lists):\n",
    "    from tensorflow import one_hot\n",
    "    max_num_tokens = get_max_num_tokens(tokens_lists)\n",
    "    indeces = get_integer_indeces_from_tokens_lists(gensim_model, tokens_lists)\n",
    "    depth = get_num_words_in_vocab(gensim_model)\n",
    "\n",
    "    length_of_one_hot_of_multiple_tokens = depth * max_num_tokens\n",
    "    encodings = -1 * np.ones([len(tokens_lists), length_of_one_hot_of_multiple_tokens])\n",
    "    for i in range(len(tokens_lists)):\n",
    "        encodings[i,:] = one_hot(indeces[i,:], depth).eval(session=sess).flatten()\n",
    "    return encodings\n",
    "\n",
    "def get_one_hot_encodings_from_tokens_lists(gensim_model, tokens_lists):\n",
    "    from tensorflow import one_hot\n",
    "    max_num_tokens = get_max_num_tokens(tokens_lists)\n",
    "    indeces = get_integer_indeces_from_tokens_lists(gensim_model, tokens_lists)\n",
    "    depth = get_num_words_in_vocab(gensim_model)\n",
    "\n",
    "    length_of_one_hot_of_multiple_tokens = depth * max_num_tokens\n",
    "    encodings = -1 * np.ones([len(tokens_lists), max_num_tokens, depth])\n",
    "    for i in range(len(tokens_lists)):\n",
    "        encodings[i,:,:] = one_hot(indeces[i,:], depth).eval(session=sess)\n",
    "    return encodings\n",
    "\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "char_to_int = dict((c,i+1) for i,c in enumerate(alphabet))\n",
    "int_to_char = dict((i+1,c) for i,c in enumerate(alphabet))\n",
    "int_to_char[0] = \"\"\n",
    "\n",
    "def convert_to_onehot(data, max_num_characters):\n",
    "    encoded_data = [char_to_int[char] for char in data]\n",
    "    one_hot = np.zeros([max_num_characters, len(alphabet)])\n",
    "    for i, value in enumerate(encoded_data):\n",
    "        one_hot[i, value] = 1\n",
    "    return np.asarray(one_hot)\n",
    "\n",
    "# def get_flattened_one_hot_encodings_from_tokens_lists(tokens_lists, max_num_tokens, max_num_characters):\n",
    "#     num_lists = len(tokens_lists)\n",
    "#     depth = len(alphabet)\n",
    "#     length_of_one_hot_of_multiple_tokens = depth * max_num_tokens * max_num_characters\n",
    "#     encodings =  np.zeros([num_lists, length_of_one_hot_of_multiple_tokens])\n",
    "#     for i in range(num_lists):\n",
    "#         words = np.zeros([max_num_tokens, max_num_characters, depth])\n",
    "#         for j in range(len(tokens_lists[i])):\n",
    "#              words[j,:,:] = convert_to_onehot(tokens_lists[i][j], max_num_characters) \n",
    "#         encodings[i,:] = np.asarray(words).flatten()\n",
    "#     return encodings\n",
    "\n",
    "def get_text_from_flattened_one_hot(flattened_one_hot, max_num_tokens, max_num_characters):\n",
    "    words = []\n",
    "    unflattened_one_hot = flattened_one_hot.reshape([max_num_tokens, max_num_characters, len(alphabet)])\n",
    "    num_words = unflattened_one_hot.shape[0]\n",
    "    for i in range(num_words):\n",
    "        word = \"\"\n",
    "        for j in range(max_num_characters):\n",
    "            index = np.argmax(unflattened_one_hot[i][j])\n",
    "            word += int_to_char[index]\n",
    "        words.append(word)\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_dir = \"/home/stevengt/Documents/code/whatwhy-data/News-Articles/all-the-news/tokens\"\n",
    "target_csv_name = \"/home/stevengt/Documents/code/whatwhy-data/News-Articles/all-the-news/wh_phrases.csv\"\n",
    "\n",
    "df = get_df_from_file(target_csv_name)\n",
    "\n",
    "for question_type in QUESTION_WORDS:\n",
    "    token_col = question_type + \" tokens\"\n",
    "    df[token_col] = df[token_col].apply(get_text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_what_tokens = df[\"what tokens\"].tolist()[:5]\n",
    "all_why_tokens = df[\"why tokens\"].tolist()[:5]\n",
    "all_tokens = all_what_tokens + all_why_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of what tokens: 4\n",
      "Max number of why tokens: 8\n",
      "Longest what word has 10 letters\n",
      "Longest why word has 9 letters\n"
     ]
    }
   ],
   "source": [
    "max_num_what_tokens = get_max_num_tokens(all_what_tokens)\n",
    "max_num_why_tokens = get_max_num_tokens(all_why_tokens)\n",
    "max_num_what_characters = get_max_token_length(all_what_tokens)\n",
    "max_num_why_characters = get_max_token_length(all_why_tokens)\n",
    "print('Max number of what tokens: %d' % max_num_what_tokens)\n",
    "print('Max number of why tokens: %d' % max_num_why_tokens)\n",
    "print('Longest what word has %d letters' % max_num_what_characters)\n",
    "print('Longest why word has %d letters' % max_num_why_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape=(3, 4, 50)\n",
      "X_test.shape=(2, 4, 50)\n",
      "y_train.shape=(3, 4, 400000)\n",
      "y_test.shape=(2, 8, 400000)\n"
     ]
    }
   ],
   "source": [
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = sk.train_test_split(all_what_tokens, all_why_tokens, test_size=0.33, random_state = 42)\n",
    "\n",
    "X_train = embed_lists_of_tokens(gensim_model, X_train_raw)\n",
    "X_test = embed_lists_of_tokens(gensim_model, X_test_raw)\n",
    "\n",
    "y_train = get_one_hot_encodings_from_tokens_lists(gensim_model, y_train_raw)\n",
    "y_test = get_one_hot_encodings_from_tokens_lists(gensim_model, y_test_raw)\n",
    "\n",
    "print(f\"X_train.shape={X_train.shape}\")\n",
    "print(f\"X_test.shape={X_test.shape}\")\n",
    "print(f\"y_train.shape={y_train.shape}\")\n",
    "print(f\"y_test.shape={y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, TimeDistributed, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def define_model(gensim_model, max_num_what_tokens, max_num_why_tokens):\n",
    "    vocabulary_size = get_num_words_in_vocab(gensim_model)\n",
    "    hidden_size = gensim_model.vector_size\n",
    "    input_shape = (max_num_what_tokens, hidden_size)\n",
    "    output_shape = (max_num_why_tokens, vocabulary_size)\n",
    "    use_dropout=False    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "#     model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    model.add(LSTM(hidden_size, return_sequences=True))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(vocabulary_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = Adam()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 4, 50)             20200     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 400000)         20400000  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 400000)         0         \n",
      "=================================================================\n",
      "Total params: 20,420,200\n",
      "Trainable params: 20,420,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model=define_model(gensim_model, max_num_what_tokens, max_num_why_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=1, batch_size=16)\n",
    "# save the model\n",
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "what_tokens = [\n",
    "    [\"hello\", \"world\", \"my\", \"name\", \"is\", \"steven\"],\n",
    "    [\"this\", \"is\", \"not\", \"some\", \"drill\"],\n",
    "    [\"this\", \"is\", \"only\", \"the\", \"test\"]\n",
    "]\n",
    "why_tokens = [\n",
    "    [\"this\", \"is\", \"some\", \"sample\", \"sentence\"],\n",
    "    [\"i\", \"learn\", \"with\", \"examples\"],\n",
    "    [\"we\", \"must\", \"figure\", \"out\", \"some\", \"solution\"]\n",
    "]\n",
    "\n",
    "max_num_what_tokens = get_max_num_tokens(what_tokens)\n",
    "max_num_why_tokens = get_max_num_tokens(why_tokens)\n",
    "\n",
    "max_num_what_characters = get_max_token_length(what_tokens)\n",
    "max_num_why_characters = get_max_token_length(why_tokens)\n",
    "\n",
    "# what_vecs = embed_lists_of_tokens(gensim_model, what_tokens)\n",
    "why_one_hot = get_flattened_one_hot_encodings_from_tokens_lists(why_tokens, max_num_why_tokens, max_num_why_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4, 50)]           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 4, 128)            58880     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4, 1872)           241488    \n",
      "=================================================================\n",
      "Total params: 300,368\n",
      "Trainable params: 300,368\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (max_num_what_tokens, gensim_model.vector_size)\n",
    "output_length = y_train.shape[1]\n",
    "model = define_model(input_shape, output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/stevengt/anaconda3/envs/whatwhy/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.fit( X_train,\n",
    "           y_train,\n",
    "           batch_size=3,\n",
    "           epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
